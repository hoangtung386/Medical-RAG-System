import os
import gradio as gr
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TextIteratorStreamer
)
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder
from threading import Thread
import numpy as np
import logging

# LOGGER SETUP
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- CONFIGURATION ---

# üéØ SINGLE MODEL APPROACH (Choose ONE)
# Option 1: GPT-OSS 20B (Powerful, fits P100 16GB in 4-bit)
MODEL_ID = "unsloth/gpt-oss-20b-bnb-4bit"

# Option 2: Gemma 2 9B (Alternative, very fast)
# MODEL_ID = "unsloth/gemma-2-9b-it-bnb-4bit"

# Option 3: Gemma 3 27B (‚ö†Ô∏è REQUIRES >24GB VRAM or CPU OFFLOAD - VERY SLOW ON P100)
# MODEL_ID = "unsloth/gemma-3-27b-it-unsloth-bnb-4bit"

# RETRIEVAL MODELS
RERANKER_MODEL = "BAAI/bge-reranker-v2-m3"
EMBEDDING_MODEL = "BAAI/bge-m3"

DB_PATH = os.path.join(os.getcwd(), "chroma_db")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# TWEAKABLE PARAMETERS
RELEVANCE_THRESHOLD = 0.3
TOP_K_RETRIEVAL = 10
TOP_K_RERANK = 5
MAX_NEW_TOKENS = 1024
TEMPERATURE = 0.2  # Lower for medical accuracy
MAX_CONTEXT_LENGTH = 4096  # Adjust based on model's context window

# SECURITY
DEFAULT_AUTH = ("admin", "123456")

print(f"Device: {DEVICE}")
print(f"Selected Model: {MODEL_ID}")

# --- INITIALIZATION ---

# 1. Load Retriever & Reranker
print("Loading Retrieval System...")
embedding_function = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={'device': DEVICE}
)

if os.path.exists(DB_PATH):
    db = Chroma(persist_directory=DB_PATH, embedding_function=embedding_function)
    retriever = db.as_retriever(search_kwargs={"k": TOP_K_RETRIEVAL})
else:
    logger.warning("Vector DB not found. Run ingest.py!")
    retriever = None

reranker = CrossEncoder(RERANKER_MODEL, device=DEVICE)

# 2. Load Main RAG Model (4-bit Quantized)
print(f"Loading RAG Model ({MODEL_ID})...")
try:
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        llm_int8_enable_fp32_cpu_offload=True # Enable if VRAM < Model Size
    )
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    # Set padding token if not set
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    print("‚úÖ Model loaded successfully.")
    print(f"   Model type: {type(model).__name__}")
    print(f"   Memory allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB")
    
except Exception as e:
    logger.error(f"Error loading model: {e}")
    raise e

# --- HELPER FUNCTIONS ---

def format_system_prompt(context):
    """
    Creates a strict medical system prompt in ENGLISH for better instruction following.
    The model is instructed to process mixed (Eng/Vi) context but respond ONLY in Vietnamese.
    """
    return f"""You are a helpful and professional Medical AI Assistant.

### INSTRUCTIONS:
1. **LANGUAGE**: The user will ask in **Vietnamese**. You MUST answer in **Vietnamese** regardless of the context's language.
2. **CONTEXT**: You are provided with medical documents below. They may be in English or Vietnamese.
   - If context is English: Translate the relevant medical knowledge to Vietnamese accurately.
   - If context is Vietnamese: Use it directly but refine the phrasing.
3. **ACCURACY**: Answer based **ONLY** on the provided context. If the answer is not in the context, state: "Th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p." (Do not hallucinate).
4. **CITATION**: Every claim must be cited with [Source X].
   - Example directly in text: "Theo nghi√™n c·ª©u [Source 1], thu·ªëc A c√≥ t√°c d·ª•ng..."
   - Or at the end of the sentence: "Thu·ªëc A c√≥ t√°c d·ª•ng ph·ª• B [Source 2]."
5. **TONE**: Professional, empathetic, and objective medical tone.

### REFERENCE CONTEXT:
{context}

### USER QUESTION (Vietnamese):"""

def truncate_context(context, max_tokens=2048):
    """Truncate context to fit within token limit."""
    tokens = tokenizer.encode(context, add_special_tokens=False)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
        context = tokenizer.decode(tokens)
        logger.warning(f"Context truncated to {max_tokens} tokens")
    return context

def chat(message, history, progress=gr.Progress()):
    """
    Simplified 3-Stage Pipeline:
    1. Retrieve
    2. Reason (Single Model - Vietnamese capable)
    3. Return
    """
    if not retriever:
        yield "‚ùå H·ªá th·ªëng ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng ch·∫°y ingest.py."
        return

    try:
        # STEP 1: Retrieval
        progress(0.2, desc="üìö ƒêang t√¨m ki·∫øm t√†i li·ªáu y khoa...")
        docs = retriever.invoke(message)
        
        if not docs:
            yield "‚ùå Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ph√π h·ª£p."
            return

        # Rerank
        doc_texts = [d.page_content for d in docs]
        scores = reranker.predict([[message, t] for t in doc_texts])
        top_indices = [i for i in np.argsort(scores)[::-1] if scores[i] > RELEVANCE_THRESHOLD][:TOP_K_RERANK]
        top_docs = [docs[i] for i in top_indices]
        
        if not top_docs:
            yield "‚ùå Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ƒë·ªß ƒë·ªô tin c·∫≠y (confidence threshold not met)."
            return

        # Build Context
        context_parts = []
        sources_list = []
        for i, doc in enumerate(top_docs):
            src = os.path.basename(doc.metadata.get('source', 'Unknown'))
            page = doc.metadata.get('page', '?')
            context_parts.append(f"[Ngu·ªìn {i+1}] {doc.page_content}\n(File: {src}, Trang: {page})")
            sources_list.append(f"- **[Ngu·ªìn {i+1}]** {src} (Trang {page})")
            
        context_str = "\n\n".join(context_parts)
        
        # Truncate context if too long
        context_str = truncate_context(context_str, max_tokens=MAX_CONTEXT_LENGTH - 1024)
        
        # STEP 2: Medical Reasoning
        progress(0.5, desc="üß† ƒêang ph√¢n t√≠ch v·ªõi AI model...")
        
        system_prompt = format_system_prompt(context_str)
        
        # Construct prompt based on model type
        if "gemma" in MODEL_ID.lower():
            # Gemma format
            full_prompt = f"{system_prompt}\n\nC√¢u h·ªèi: {message}\n\nTr·∫£ l·ªùi:"
        elif "qwen" in MODEL_ID.lower():
            # Qwen format
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": message}
            ]
            full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        else:
            # Generic format
            full_prompt = f"{system_prompt}\n\nC√¢u h·ªèi: {message}\n\nTr·∫£ l·ªùi:"
        
        inputs = tokenizer(full_prompt, return_tensors="pt", truncation=True, max_length=MAX_CONTEXT_LENGTH).to(DEVICE)

        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        generate_kwargs = dict(
            **inputs,
            streamer=streamer,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=TEMPERATURE,
            top_p=0.9,
            repetition_penalty=1.15,
        )
        
        t = Thread(target=model.generate, kwargs=generate_kwargs)
        t.start()
        
        # Stream response
        response = ""
        for token in streamer:
            response += token
            # Real-time streaming to UI
            yield response + "\n\n‚è≥ ƒêang t·∫°o c√¢u tr·∫£ l·ªùi..."
        
        # STEP 3: Final Output with Sources
        final_output = (
            f"{response.strip()}\n\n"
            f"---\n### üìö Ngu·ªìn tham kh·∫£o:\n" + "\n".join(sources_list)
        )
        
        yield final_output

    except Exception as e:
        logger.error(f"Pipeline failed: {e}", exc_info=True)
        yield f"‚ùå L·ªói h·ªá th·ªëng: {str(e)}"

# --- UI SETUP ---
with gr.Blocks(theme=gr.themes.Soft(), title="Medical RAG System", fill_height=True) as demo:
    gr.Markdown(
        f"# üè• H·ªá th·ªëng Tr·ª£ l√Ω Y khoa AI\n"
        f"**Model:** {MODEL_ID.split('/')[-1]}\n"
        f"**Ki·∫øn tr√∫c:** Single-Model RAG (No Translation Bridge)\n"
        f"**GPU:** Tesla P100 16GB"
    )
    
    with gr.Accordion("‚ÑπÔ∏è L∆∞u √Ω quan tr·ªçng", open=False):
        gr.Markdown(
            "- ‚úÖ **Kh√¥ng c√≤n translation bridge** - c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n\n"
            "- ‚ö° **T·ªëc ƒë·ªô nhanh h∆°n** - ch·ªâ 1 model duy nh·∫•t\n"
            "- üáªüá≥ **Native Vietnamese** - hi·ªÉu ti·∫øng Vi·ªát t·ª± nhi√™n\n"
            "- ‚öïÔ∏è **Ch·ªâ mang t√≠nh tham kh·∫£o** - lu√¥n tham kh·∫£o b√°c sƒ©"
        )

    gr.ChatInterface(
        fn=chat,
        description="ƒê·∫∑t c√¢u h·ªèi y khoa b·∫±ng ti·∫øng Vi·ªát. H·ªá th·ªëng s·∫Ω t√¨m ki·∫øm v√† ph√¢n t√≠ch t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi.",
        examples=[
            "Tri·ªáu ch·ª©ng c·ªßa b·ªánh ti·ªÉu ƒë∆∞·ªùng type 2 l√† g√¨?",
            "T√°c d·ª•ng ph·ª• c·ªßa thu·ªëc aspirin?",
            "L√†m sao ƒë·ªÉ ph√≤ng ng·ª´a b·ªánh tim m·∫°ch?",
            "Ch·∫ø ƒë·ªô ƒÉn cho ng∆∞·ªùi huy·∫øt √°p cao?"
        ],
        retry_btn="üîÑ Th·ª≠ l·∫°i",
        undo_btn="‚Ü©Ô∏è Ho√†n t√°c",
        clear_btn="üóëÔ∏è X√≥a h·∫øt"
    )
    
    gr.Markdown(
        "\n---\n"
        "**‚ö†Ô∏è C·∫£nh b√°o y t·∫ø:** Th√¥ng tin t·ª´ h·ªá th·ªëng ch·ªâ mang t√≠nh tham kh·∫£o. "
        "Kh√¥ng thay th·∫ø cho ch·∫©n ƒëo√°n v√† ƒëi·ªÅu tr·ªã c·ªßa b√°c sƒ© chuy√™n khoa."
    )

if __name__ == "__main__":
    demo.queue().launch(
        share=True, 
        server_name="0.0.0.0", 
        auth=DEFAULT_AUTH,
        show_error=True
    )
    