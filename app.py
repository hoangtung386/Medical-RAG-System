import os
import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer, BitsAndBytesConfig
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder
from threading import Thread
import numpy as np
import logging

# LOGGER SETUP
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# UPGRADED CONFIGURATION
# NEW: Ministral-3-8B-Reasoning - MUCH better Vietnamese support!
MODEL_ID = "mistralai/Ministral-3-8B-Reasoning-2512"  
# This model has excellent multilingual capabilities and follows system prompts strictly

RERANKER_MODEL = "BAAI/bge-reranker-v2-m3"
EMBEDDING_MODEL = "BAAI/bge-m3"  # Keep the upgraded embedding

DB_PATH = os.path.join(os.getcwd(), "chroma_db")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Tweakable Parameters
RELEVANCE_THRESHOLD = 0.3
MAX_HISTORY_LEN = 10
MAX_INPUT_LEN = 2000
MIN_INPUT_LEN = 5
TOP_K_RETRIEVAL = 10
TOP_K_RERANK = 8
TEMPERATURE = 0.7  # Mistral recommends 0.7 for reasoning models
MAX_NEW_TOKENS = 768

# Security
DEFAULT_AUTH = ("admin", "123456")  # ‚ö†Ô∏è CHANGE THIS!

MEDICAL_DISCLAIMER = """
### C·∫¢NH B√ÅO Y T·∫æ QUAN TR·ªåNG
1. **M·ª•c ƒë√≠ch tham kh·∫£o**: C√¥ng c·ª• n√†y ch·ªâ cung c·∫•p th√¥ng tin y t·∫ø t·ªïng qu√°t ƒë·ªÉ tham kh·∫£o.
2. **Kh√¥ng thay th·∫ø b√°c sƒ©**: Th√¥ng tin **KH√îNG** c√≥ gi√° tr·ªã ch·∫©n ƒëo√°n, ƒëi·ªÅu tr·ªã hay t∆∞ v·∫•n y khoa.
3. **Mi·ªÖn tr·ª´ tr√°ch nhi·ªám**: Ng∆∞·ªùi d√πng t·ª± ch·ªãu tr√°ch nhi·ªám khi s·ª≠ d·ª•ng th√¥ng tin. Lu√¥n tham kh·∫£o √Ω ki·∫øn b√°c sƒ©.

Powered by: Ministral-3-8B-Reasoning (Multilingual + Reasoning) + BGE-M3 (SOTA Embedding)
"""

print(f"Device: {DEVICE}")

# INITIALIZATION

# Load Retriever with BGE-M3
print(f"Loading Vector Database with {EMBEDDING_MODEL}...")
embedding_function = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={'device': DEVICE if DEVICE == 'cuda' else 'cpu'}
)

if not os.path.exists(DB_PATH):
    logger.warning(f"Vector DB not found at {DB_PATH}. Please run ingest.py first.")
    db = None
else:
    db = Chroma(persist_directory=DB_PATH, embedding_function=embedding_function)

if db:
    retriever = db.as_retriever(search_kwargs={"k": TOP_K_RETRIEVAL})
else:
    retriever = None

# Load Reranker
print(f"Loading Reranker {RERANKER_MODEL}...")
reranker = CrossEncoder(RERANKER_MODEL, device=DEVICE)

# Load Ministral-3-8B-Reasoning Model
print(f"Loading Reasoning Model {MODEL_ID}...")

try:
    # Ministral supports 4-bit quantization
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True  # Extra optimization
    )
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    
    # Important: Set pad_token if not set (Ministral sometimes needs this)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=quantization_config if DEVICE == "cuda" else None,
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
        device_map="auto" if DEVICE == "cuda" else None,
        trust_remote_code=True
    )
    
    if DEVICE == "cpu":
        logger.warning("Running on CPU! This will be slow.")
        model.to("cpu")
    
    print("Model loaded successfully!")
        
except Exception as e:
    logger.error(f"Error loading {MODEL_ID}: {e}")
    raise e

# HELPER FUNCTIONS

def validate_input(message):
    """Checks input length and validity."""
    if not message or len(message.strip()) < MIN_INPUT_LEN:
        return "C√¢u h·ªèi qu√° ng·∫Øn. Vui l√≤ng nh·∫≠p chi ti·∫øt h∆°n."
    if len(message) > MAX_INPUT_LEN:
        return f"C√¢u h·ªèi qu√° d√†i (>{MAX_INPUT_LEN} k√Ω t·ª±). Vui l√≤ng r√∫t g·ªçn."
    return None

def format_prompt_for_ministral(message, history, context):
    """
    OPTIMIZED PROMPT for Ministral-3-8B-Reasoning
    
    Key changes from DeepSeek-R1 version:
    1. Stricter language control (MUST respond in Vietnamese)
    2. Simpler instructions (Ministral is smaller, less verbose)
    3. Emphasize system prompt adherence (Ministral's strength)
    """
    
    # Ministral recommends concise system prompts
    system_prompt = (
        "You are a medical information assistant. You MUST follow these rules:\n\n"
        
        "**CRITICAL - LANGUAGE RULE:**\n"
        "- Your ENTIRE response MUST be in Vietnamese only\n"
        "- Never mix English, French, or other languages in your answer\n"
        "- Translate all medical terms to Vietnamese\n"
        "- If you don't know the Vietnamese term, describe it in Vietnamese\n\n"
        
        "**RESPONSE STRUCTURE:**\n"
        "1. Answer the question directly in Vietnamese\n"
        "2. Cite sources using [Source X] format for every claim\n"
        "3. If sources conflict, present all viewpoints\n"
        "4. If information is insufficient, say 'Th√¥ng tin ch∆∞a ƒë·∫ßy ƒë·ªß'\n\n"
        
        "**SAFETY:**\n"
        "- Never provide diagnosis or treatment recommendations\n"
        "- Always encourage consulting healthcare professionals\n"
        "- Mention risks and contraindications when relevant\n\n"
        
        "Context includes numbered sources: [Source 1], [Source 2], etc."
    )
    
    messages = [{"role": "system", "content": system_prompt}]
    
    # Add history (limited)
    for human, ai in history[-MAX_HISTORY_LEN:]:
        messages.append({"role": "user", "content": human})
        if ai:
            messages.append({"role": "assistant", "content": ai})
    
    # Add current message with context
    # Important: Remind the model again about Vietnamese
    content_with_context = (
        f"**T√†i li·ªáu y khoa (Medical Context):**\n{context}\n\n"
        f"**C√¢u h·ªèi (Question):**\n{message}\n\n"
        f"**QUAN TR·ªåNG:** Tr·∫£ l·ªùi HO√ÄN TO√ÄN b·∫±ng ti·∫øng Vi·ªát. Kh√¥ng l·∫´n l·ªôn ng√¥n ng·ªØ kh√°c."
    )
    messages.append({"role": "user", "content": content_with_context})
    
    return messages

def chat(message, history, progress=gr.Progress()):
    """
    Main chat logic with Ministral-3-8B-Reasoning
    """
    error_msg = validate_input(message)
    if error_msg:
        yield error_msg
        return

    if not retriever:
        yield "L·ªói: C∆° s·ªü d·ªØ li·ªáu ch∆∞a s·∫µn s√†ng. Vui l√≤ng ch·∫°y ingest.py tr∆∞·ªõc."
        return

    try:
        progress(0.1, desc="ƒêang t√¨m ki·∫øm t√†i li·ªáu...")
        
        # 1. Retrieve
        docs = retriever.invoke(message)
        if not docs:
            yield "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."
            return

        progress(0.4, desc="ƒêang ƒë√°nh gi√° ƒë·ªô li√™n quan...")
        
        # 2. Rerank
        doc_texts = [doc.page_content for doc in docs]
        top_docs = []
        
        if doc_texts:
            pairs = [[message, doc_text] for doc_text in doc_texts]
            scores = reranker.predict(pairs)
            
            sorted_indices = np.argsort(scores)[::-1]
            
            top_k_indices = []
            for i in sorted_indices:
                if scores[i] > RELEVANCE_THRESHOLD:
                    top_k_indices.append(i)
                if len(top_k_indices) >= TOP_K_RERANK:
                    break
            
            top_docs = [docs[i] for i in top_k_indices]
        
        if not top_docs:
            yield "Xin l·ªói, kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë·ªß ƒë·ªô tin c·∫≠y (>30%) ƒë·ªÉ tr·∫£ l·ªùi."
            return

        progress(0.6, desc="ƒêang suy lu·∫≠n v·ªõi Ministral...")

        # 3. Context Construction
        context_pieces = []
        sources_list = []
        
        for i, doc in enumerate(top_docs):
            source_path = doc.metadata.get('source', 'Unknown File')
            filename = os.path.basename(source_path)
            
            raw_page = doc.metadata.get('page', -1)
            if isinstance(raw_page, int) and raw_page >= 0:
                page_display = raw_page + 1
            else:
                page_display = "Unknown"
            
            context_pieces.append(
                f"[Source {i+1}]: {doc.page_content}\n"
                f"(T√†i li·ªáu: {filename}, Trang {page_display})"
            )
            sources_list.append(f"- [Source {i+1}]: {filename} (Trang {page_display})")
            
        context = "\n\n".join(context_pieces)
        
        # 4. Generate with Ministral
        messages = format_prompt_for_ministral(message, history, context)
        
        # Tokenize with Ministral's chat template
        try:
            inputs = tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            ).to(model.device)
        except Exception as e:
            # Fallback if chat template fails
            logger.warning(f"Chat template failed: {e}. Using manual formatting.")
            prompt_text = "\n\n".join([
                f"{'System' if m['role']=='system' else m['role'].capitalize()}: {m['content']}" 
                for m in messages
            ]) + "\n\nAssistant:"
            inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)
        
        streamer = TextIteratorStreamer(
            tokenizer,
            timeout=30.0,
            skip_prompt=True,
            skip_special_tokens=True
        )
        
        generate_kwargs = dict(
            **inputs,
            streamer=streamer,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=TEMPERATURE,
            top_p=0.9,
            repetition_penalty=1.1,  # Prevent repetition
        )
        
        t = Thread(target=model.generate, kwargs=generate_kwargs)
        t.start()
        
        partial_response = ""
        for new_token in streamer:
            partial_response += new_token
            yield partial_response

        # 5. Append Sources
        if sources_list and "T√†i li·ªáu tham kh·∫£o" not in partial_response:
            final_response = (
                partial_response + 
                "\n\n---\n**üìö T√†i li·ªáu tham kh·∫£o:**\n" + 
                "\n".join(sources_list)
            )
            yield final_response
        else:
            yield partial_response

    except Exception as e:
        logger.error(f"Error in chat: {e}", exc_info=True)
        yield f"ƒê√£ x·∫£y ra l·ªói h·ªá th·ªëng: {str(e)}"

# --- UI SETUP ---
with gr.Blocks(theme=gr.themes.Soft(), title="Medical RAG Assistant", fill_height=True) as demo:
    gr.Markdown(
        f"# Medical RAG Assistant\n"
        f"**Model:** Ministral-3-8B-Reasoning (Multilingual + Reasoning)\n"
        f"**Embedding:** BGE-M3 (1024-dim, 8K context)\n"
        f"**Pipeline:** Retrieve({TOP_K_RETRIEVAL}) ‚Üí Rerank({TOP_K_RERANK}) ‚Üí Reason ‚Üí Respond in Vietnamese"
    )
    
    with gr.Accordion("‚ö†Ô∏è ƒê·ªåC K·ª∏: C·∫¢NH B√ÅO Y T·∫æ", open=False):
        gr.Markdown(MEDICAL_DISCLAIMER)
    
    gr.ChatInterface(
        fn=chat,
        description="H·ªá th·ªëng tra c·ª©u y khoa v·ªõi kh·∫£ nƒÉng suy lu·∫≠n v√† tr·∫£ l·ªùi HO√ÄN TO√ÄN b·∫±ng ti·∫øng Vi·ªát.",
        examples=[
            "Tri·ªáu ch·ª©ng c·ªßa b·ªánh ti·ªÉu ƒë∆∞·ªùng type 2 l√† g√¨?",
            "So s√°nh metformin v√† insulin cho ƒëi·ªÅu tr·ªã ti·ªÉu ƒë∆∞·ªùng?",
            "T√°c d·ª•ng ph·ª• c·ªßa aspirin l√† g√¨?",
            "Bi·∫øn ch·ª©ng c·ªßa ph·∫´u thu·∫≠t thay kh·ªõp h√°ng?",
            "C√°ch ph√≤ng ng·ª´a b·ªánh tim m·∫°ch ·ªü ng∆∞·ªùi tr√™n 50 tu·ªïi?"
        ],
        fill_height=True,
    )
    
    with gr.Accordion("üí° Tips s·ª≠ d·ª•ng", open=False):
        gr.Markdown("""
### C√°ch h·ªèi hi·ªáu qu·∫£:
- **T·ªët:** "Tri·ªáu ch·ª©ng c·ªßa b·ªánh ti·ªÉu ƒë∆∞·ªùng type 2 l√† g√¨? Gi·∫£i th√≠ch nguy√™n nh√¢n."
- **K√©m:** "ti·ªÉu ƒë∆∞·ªùng" (qu√° ng·∫Øn, kh√¥ng r√µ r√†ng)

### H·ªá th·ªëng n√†y:
- Tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát (ƒë√£ fix l·ªói l·∫´n l·ªôn ng√¥n ng·ªØ)
- Cung c·∫•p tr√≠ch d·∫´n r√µ r√†ng t·ª´ t√†i li·ªáu
- C√≥ kh·∫£ nƒÉng suy lu·∫≠n logic cho c√¢u h·ªèi ph·ª©c t·∫°p
- KH√îNG thay th·∫ø b√°c sƒ© - ch·ªâ ƒë·ªÉ tham kh·∫£o th√¥ng tin

### Th·ªùi gian x·ª≠ l√Ω:
- C√¢u h·ªèi ƒë∆°n gi·∫£n: ~5-8 gi√¢y
- C√¢u h·ªèi ph·ª©c t·∫°p: ~10-15 gi√¢y (model ƒëang "suy nghƒ©")
        """)

if __name__ == "__main__":
    demo.queue().launch(
        share=True,
        server_name="0.0.0.0",
        auth=DEFAULT_AUTH,
        debug=True,
        show_error=True
    )
